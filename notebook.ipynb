{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 이 데이터는 CSV 파일 './file/data.csv'에서 추출한 것입니다. 각 행에는 \"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\" 열이 있습니다. 이 데이터는 Facturierung (영수증) 관련 정보입니다. 각 행은 하나의 거래에 대한 정보를 담고 있습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' 이 데이터는 CSV 파일 \\'./file/data.csv\\'에서 추출한 것입니다. 각 행에는 \"InvoiceNo\", \"StockCode\", \"Description\", \"Quantity\", \"InvoiceDate\", \"UnitPrice\", \"CustomerID\", \"Country\" 열이 있습니다. 이 데이터는 Facturierung (영수증) 관련 정보입니다. 각 행은 하나의 거래에 대한 정보를 담고 있습니다.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings import OllamaEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.1,\n",
    "    callbacks=[\n",
    "        StreamingStdOutCallbackHandler(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "loader = CSVLoader(\"./file/data.csv\")\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mistral:latest\"\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorStore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorStore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a data analyst. Answer questions using only the given context. Answer in Korean. Don't give me english answer. Don't try to translate the name of the column used in the data. Explain briefly about what the data is about first. And then explain about the data overall.  If you don't know the answer, just say you don't know. Do NOT make it up. And also, do not talk about anything else:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = {\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "    } | prompt | llm\n",
    "\n",
    "chain.invoke(\"Tell me about the data. Answer in Korean.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
